{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference modes\n",
    "\n",
    "The most usual approach of using neural networks for computer vision is to capture an image and feed the whole image to the neural network for inference. However, sometimes more complex logic is necessary, depending on the application. Some examples from our experience:\n",
    "* Crop off the boundaries of the image and only run inference on the inner part.\n",
    "* Run over the input image with a sliding window, run inference on each fragment and then aggregate to get the overall result.\n",
    "* In a pipeline setting, run inference only on specially selected regions. A typical example of this is first running a detection model on the whole image to get rough locations of the objects, and then run a segmentation model to find fine-grained contours on the objects *only within the locations found by the detector*.\n",
    "\n",
    "The neural networks available out of the box in Supervisely all support the above *inferfence modes* with no code modifications. Only inference config for a given inference task need to be changed. Moreover, since this advanced logic is independent of the specific neural network being used, we have factored it out as a part of `supervisely_lib` SDK. It is trivial to integrate the available inference modes with your custom models also, as long as you rely on our `SingleImageInferenceBase` base class for inference. See [our guide on how to integrate your custom neural network with the Supervisely platform](https://github.com/supervisely/supervisely/blob/master/help/tutorials/02_custom_neural_net_plugin/custom_nn_plugin.md).\n",
    "\n",
    "In this tutorial we will go over the examples of available inference modes for neural networks in Supervisely.\n",
    "\n",
    "One important thing to keep in mind is that all the inference modes produce labels in the context of the *full original image*. So, for example, if the original image is cropped before passing it to the neural network, the inference results from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference config structure\n",
    "\n",
    "With all the Supervisely built-in neural networks, the inference mode settings and the settings of the specific network itself are decomposed into different sections of the overall inference task config as follows:\n",
    "```python\n",
    "{\n",
    "  \"model\": {\n",
    "    # Config of the specific neural network.\n",
    "    # Detection thresholds, other runtime options.\n",
    "  },\n",
    "  \"mode\": {\n",
    "    # Inference mode config. Options used here are general\n",
    "    # and do not depend on the specific neural network being used.\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full image inference\n",
    "\n",
    "The most basic inference mode is to pass the whole incoming image to the neural network for inference. Example config:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"model\": {\n",
    "    \"gpu_device\": 0  # Use GPU #0 for this model.\n",
    "  },\n",
    "  \"mode\": {\n",
    "\n",
    "    # Inference mode name, mandatory field.\n",
    "    # \"full_image\" is the most basic mode, where all the image is\n",
    "    # passed on to the neural net.\n",
    "    \"name\": \"full_image\",\n",
    "    \n",
    "    # Mode-specific settings. For example, here we describe how to\n",
    "    # post-process the labels returned by the model depending on\n",
    "    # the object class.\n",
    "\n",
    "    \"model_classes\": {\n",
    "      # This section is actually common to all of the inference modes.\n",
    "    \n",
    "      # Add a suffix to all of the class names returned by the model.\n",
    "      # This helps distinguish model labels from original labels when\n",
    "      # using a labeled dataset as input.\n",
    "      \"add_suffix\": \"_unet\",\n",
    "      \n",
    "      # Save all the labels produced by the models. Alternatively,\n",
    "      # here one can specify a whitelist of class names to be saved,\n",
    "      # e.g. [\"car\", \"person\"].\n",
    "      \"save_classes\": \"__all__\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI (fixed crop) inference\n",
    "\n",
    "Sometimes we know that only a certain part of the input images is relevant for inference. For example, if a camera is fixed on the windshield of a car, we know that the top part of the image is the sky, where we are not going to have pedestrians, so it makes sense to only run the pedestrian detector on the bottom part of the image. In such cases we want to crop off a *fixed boundary* of the input image before passing it on to the neural network. This logic is handled by the `roi` inference mode.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"model\": {},\n",
    "  \"mode\": {\n",
    "\n",
    "    # Crop the image boundaries before inference.\n",
    "    \"name\": \"roi\",\n",
    "    \n",
    "    # Class renaming and filtering settings.\n",
    "    # See \"Full image inference\" example for details.\n",
    "    \"model_classes\": {\n",
    "    }\n",
    "   \n",
    "    # Cropping settings.\n",
    "    # How much to crop from every side of the image.\n",
    "    \"bounds\": {\n",
    "      # The amount can be specified in pixels or\n",
    "      # as a percentage of the corresponding dimension.\n",
    "      \"left\": \"100px\",\n",
    "      \"right\": \"50px\",\n",
    "      \"top\": \"20%\",\n",
    "      \"bottom\": \"20%\"\n",
    "    },\n",
    "\n",
    "    # Whether to add a bounding box of the cropped region\n",
    "    # of interest as a separate label in the results.\n",
    "    \"save\": false,\n",
    "\n",
    "    # If saving the cropped region bounding box, which\n",
    "    # class name to use.\n",
    "    \"class_name\": 'inference_roi'\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window inference\n",
    "\n",
    "#### Segmentation\n",
    "\n",
    "In segmentation sliding window mode, the per-pixels class scores are summed up over all the sliding windows affecting the given pixel, and then the class with the maximum total score is taken as a label. Example config:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"model\": {},\n",
    "  \"mode\": {\n",
    "\n",
    "    # Go over the original image with a sliding window,\n",
    "    # sum up per class segmentation scores and find the\n",
    "    # class with the highest score for every pixel.\n",
    "    \"name\": \"sliding_window\",\n",
    "    \n",
    "    # Class renaming and filtering settings.\n",
    "    # See \"Full image inference\" example for details.\n",
    "    \"model_classes\": {\n",
    "    }\n",
    "   \n",
    "    # Sliding window parameters.\n",
    "    \n",
    "    # Width and height in pixels.\n",
    "    # Cannot be larger than the original image.\n",
    "    \"window\": {\n",
    "      \"width\": 128,\n",
    "      \"height\": 128,\n",
    "    },\n",
    "\n",
    "    # Minimum overlap for each dimension. The last\n",
    "    # window in every dimension may have higher overlap\n",
    "    # with the previous one if necessary to fit the whole\n",
    "    # window within the original image.\n",
    "    \"min_overlap\": {\n",
    "      \"x\": 0,\n",
    "      \"y\": 0,\n",
    "    },\n",
    "    \n",
    "    # Whether to save each sliding window instance as a\n",
    "    # bounding box rectangle.\n",
    "    \"save\": false,\n",
    "\n",
    "    # If saving the sliding window bounding boxes, which\n",
    "    # class name to use.\n",
    "    \"class_name\": 'sliding_window_bbox',\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Detection\n",
    "With sliding windows for detection, instead of summing up per-pixel class scores, we accumulate the detection results (as rectangular bounding boxes). Optionally, non-maximum suppression is done for the final results. All of the options of the segmentation sliding window config apply, and an extra section with non-maximum suppression config is added:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"model\": {},\n",
    "  \"mode\": {\n",
    "\n",
    "    \"name\": \"sliding_window_det\",\n",
    "      \n",
    "    # All the sliding window options from the segmentation\n",
    "    # sliding window config also apply here.\n",
    "    \n",
    "    \"nms_after\": {\n",
    "      \n",
    "      # Whether to run non-maximum suppression after accumulating\n",
    "      # all the detection results from the sliding windows.\n",
    "      \"enable\": true,\n",
    "      \n",
    "      # Intersection over union threshold above which the same-class\n",
    "      # detection labels are considered to be significantly inersected\n",
    "      # for non-maximum suppression.\n",
    "      \"iou_threshold\": 0.2,\n",
    "      \n",
    "      # Tag name from which to read detection confidence by which we\n",
    "      # rank the detections. This tag must be added by the model to\n",
    "      # every detection label.\n",
    "      \"confidence_tag_name\": \"confidence\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining - bounding boxes ROI inference\n",
    "\n",
    "In this mode, the neural network inference is invoked on subimages, defined by bounding boxes of existing labels. Those existing labels may come both from a previous run of another neural network (which allows us to pipeline the models), or from manual annotations. Example config:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"model\": {},\n",
    "  \"mode\": {\n",
    "\n",
    "    # Run inference within bounding boxes of existing labels.\n",
    "    \"name\": \"bboxes\",\n",
    "    \n",
    "    # Class renaming and filtering settings.\n",
    "    # See \"Full image inference\" example for details.\n",
    "    \"model_classes\": {\n",
    "    }\n",
    "   \n",
    "    # Filter the source bounding boxes by their object class name.\n",
    "    # Use \"__all__\" to pass through all the classes or a whitelist\n",
    "    # of classes like [\"car\", \"person\"].\n",
    "    \"from_classes\": \"__all__\",\n",
    "    \n",
    "    # Padding settings for the source bounding boxes.\n",
    "    \"padding\": {\n",
    "      # Padding can be set in pixels or percents of the respective side\n",
    "      \"left\": \"20px\",\n",
    "      # Negative numbers mean crop instead of pad.\n",
    "      \"right\": \"-30px\",\n",
    "      \"top\": \"20%\",\n",
    "      \"bottom\": \"10%\"\n",
    "    },\n",
    "    \n",
    "    # Whether to save the bounding boxes that were used for selecting subimages\n",
    "    # for inference.\n",
    "    \"save\": true,\n",
    "\n",
    "    # Because the input bounding boxes may have originated from multiple\n",
    "    # object classes, we do not want to assign the same class name to them all.\n",
    "    # Instead, append a given suffix to the original class name to get the\n",
    "    # saved bounding box class name.\n",
    "    \"add_suffix\": \"_input_bbox\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
